{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Recommendation System with Graph Neural Networks\n",
    "\n",
    "**Project Goal:** To build a recommendation system using Graph Neural Networks (GNNs) to predict user engagement with items. We use the MovieLens 1M dataset and explore models like LightGCN and GraphSAGE.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Setup, Data Loading, and Initial Exploration\n",
    "\n",
    "**Objective:** Load necessary libraries and the dataset. Perform initial exploration to understand its structure and prepare for graph construction.\n",
    "\n",
    "**Dataset:** MovieLens 1M. Contains 1 million ratings from ~6,000 users on ~4,000 movies.\n",
    "\n",
    "**Libraries:** PyTorch, PyTorch Geometric, Pandas, NumPy, Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "PyTorch Geometric version: 2.6.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "import os # Added for path joining\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.nn import SAGEConv, GATConv, GCNConv, LightGCN, to_hetero\n",
    "from torch_geometric.loader import LinkNeighborLoader # For link prediction tasks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load MovieLens 1M Data\n",
    "\n",
    "We'll load the `ratings.dat` file, which contains `UserID::MovieID::Rating::Timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load MovieLens 1M dataset from: C:\\Users\\amanp\\OneDrive - purdue.edu\\Spring 2025\\Projects\\GNN recommendation system\\ml-1m\\ratings.dat\n",
      "Dataset shape: (1000209, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0        1     1193       5  978300760\n",
       "1        1      661       3  978302109\n",
       "2        1      914       3  978301968\n",
       "3        1     3408       4  978300275\n",
       "4        1     2355       5  978824291"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the base path to your MovieLens 1M directory\n",
    "base_data_dir = r\"C:\\Users\\amanp\\OneDrive - purdue.edu\\Spring 2025\\Projects\\GNN recommendation system\\ml-1m\"\n",
    "ratings_filename = 'ratings.dat'\n",
    "data_path = os.path.join(base_data_dir, ratings_filename)\n",
    "\n",
    "print(f\"Attempting to load MovieLens 1M dataset from: {data_path}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path, sep='::', names=['user_id', 'item_id', 'rating', 'timestamp'], engine='python', encoding='latin-1')\n",
    "except FileNotFoundError:\n",
    "    print(f\"MovieLens 1M dataset not found at '{data_path}'.\")\n",
    "    print(\"Please ensure the path is correct and the 'ratings.dat' file exists there.\")\n",
    "    print(\"You can download MovieLens 1M from: https://grouplens.org/datasets/movielens/1m/\")\n",
    "    # As a fallback, create a small dummy dataframe for the rest of the notebook to run\n",
    "    # This is NOT representative of the actual dataset and is only for code execution demonstration\n",
    "    data = {'user_id': [1, 1, 2, 2, 3, 3, 4, 1, 5, 5, 2, 6, 7, 7, 8, 8, 9, 9, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] * 100, # Increased dummy data size slightly\n",
    "            'item_id': [101, 102, 101, 103, 102, 104, 103, 105, 104, 106, 105, 107, 106, 108, 107, 109, 108, 110, 109, 101, 103, 105, 102, 106, 101, 107, 104, 108, 102, 110] * 100,\n",
    "            'rating': [5, 3, 4, 2, 5, 1, 4, 3, 5, 2, 4, 5, 3, 4, 2, 5, 1, 4, 3, 5, 2, 4, 5, 3, 4, 2, 5, 1, 4, 3] * 100,\n",
    "            'timestamp': [978300000 + i for i in range(3000)]}\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"\\nWARNING: Using a small dummy dataset as MovieLens 1M was not found. Results will not be meaningful.\")\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Preprocessing\n",
    "\n",
    "* **User and Item ID Encoding:** GNNs typically work with zero-indexed integer IDs. We'll use `LabelEncoder`.\n",
    "* **Implicit Feedback:** We'll consider all interactions as positive edges in the graph, as both LightGCN and our GraphSAGE variant will model the existence of an interaction for collaborative filtering.\n",
    "* **Train-Test Split:** Crucial for evaluation. A temporal split is ideal for recommendation (train on past, test on future). For simplicity here, we'll do a random split, but acknowledge temporal splitting as best practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 6040\n",
      "Number of unique items: 3706\n",
      "Number of interactions: 1000209\n",
      "Sparsity: 0.9553\n",
      "Training interactions: 800167\n",
      "Testing interactions: 200042\n"
     ]
    }
   ],
   "source": [
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "df['user_idx'] = user_encoder.fit_transform(df['user_id'])\n",
    "df['item_idx'] = item_encoder.fit_transform(df['item_id'])\n",
    "\n",
    "num_users = df['user_idx'].nunique()\n",
    "num_items = df['item_idx'].nunique()\n",
    "num_interactions = len(df)\n",
    "\n",
    "print(f\"Number of unique users: {num_users}\")\n",
    "print(f\"Number of unique items: {num_items}\")\n",
    "print(f\"Number of interactions: {num_interactions}\")\n",
    "print(f\"Sparsity: {1 - num_interactions / (num_users * num_items):.4f}\")\n",
    "\n",
    "# We'll split the interactions to create a training set of edges and a test set of edges.\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df['user_idx']) # Stratify by user to ensure users are in both sets\n",
    "\n",
    "print(f\"Training interactions: {len(train_df)}\")\n",
    "print(f\"Testing interactions: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Graph Construction\n",
    "\n",
    "**Objective:** Represent the user-item interactions as a bipartite graph. For use with GNNs like LightGCN and GraphSAGE in PyTorch Geometric, we often convert this to a homogeneous graph structure where user and item nodes are part of a single node set, distinguished by their ID range.\n",
    "\n",
    "User and item features are usually their learnable embeddings, especially for LightGCN. GraphSAGE can also incorporate explicit node features if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full graph data for GNNs:\n",
      "Data(edge_index=[2, 2000418], num_nodes=9746)\n",
      "\n",
      "Shape of training positive edges: torch.Size([2, 800167])\n",
      "Shape of testing positive edges: torch.Size([2, 200042])\n"
     ]
    }
   ],
   "source": [
    "# We will construct a single graph using all interactions for message passing.\n",
    "# The train_df and test_df will be used to define positive pairs for training and evaluation.\n",
    "\n",
    "# Create edge index for the full graph (user-item interactions)\n",
    "user_nodes_full = torch.tensor(df['user_idx'].values, dtype=torch.long)\n",
    "item_nodes_full = torch.tensor(df['item_idx'].values, dtype=torch.long)\n",
    "\n",
    "# For a bipartite graph to be used with these GNNs in PyG, we represent it as a homogeneous graph\n",
    "# where user nodes are [0, num_users-1] and item nodes are [num_users, num_users+num_items-1].\n",
    "src_full = user_nodes_full\n",
    "dst_full = item_nodes_full + num_users # Offset item indices\n",
    "\n",
    "# Create bidirectional edges\n",
    "edge_index_full = torch.stack([\n",
    "    torch.cat([src_full, dst_full]),\n",
    "    torch.cat([dst_full, src_full])\n",
    "], dim=0)\n",
    "\n",
    "graph_data = Data(edge_index=edge_index_full, num_nodes=num_users + num_items)\n",
    "print(\"Full graph data for GNNs:\")\n",
    "print(graph_data)\n",
    "\n",
    "# Prepare train and test edges (positive interactions)\n",
    "train_user_nodes = torch.tensor(train_df['user_idx'].values, dtype=torch.long)\n",
    "train_item_nodes = torch.tensor(train_df['item_idx'].values, dtype=torch.long)\n",
    "train_pos_edge_index = torch.stack([train_user_nodes, train_item_nodes], dim=0)\n",
    "\n",
    "test_user_nodes = torch.tensor(test_df['user_idx'].values, dtype=torch.long)\n",
    "test_item_nodes = torch.tensor(test_df['item_idx'].values, dtype=torch.long)\n",
    "test_pos_edge_index = torch.stack([test_user_nodes, test_item_nodes], dim=0)\n",
    "\n",
    "print(f\"\\nShape of training positive edges: {train_pos_edge_index.shape}\")\n",
    "print(f\"Shape of testing positive edges: {test_pos_edge_index.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: GNN Models\n",
    "\n",
    "**Objective:** Implement GNN models for recommendation. We'll start with LightGCN and then add GraphSAGE as an alternative advanced model.\n",
    "\n",
    "### 3.1 LightGCN (Light Graph Convolution Network)\n",
    "\n",
    "**Rationale:**\n",
    "* **Simplicity:** Removes feature transformations and non-linearities from traditional GCNs.\n",
    "* **Effectiveness:** Strong performance for collaborative filtering.\n",
    "* **Core Idea:** Learns embeddings by linearly propagating them on the user-item graph. Final embedding is a weighted sum from different propagation layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import LightGCN # Make sure this is imported\n",
    "\n",
    "# Assume num_users and num_items are defined from your data preprocessing, e.g.:\n",
    "# num_users = df['user_idx'].nunique()\n",
    "# num_items = df['item_idx'].nunique()\n",
    "\n",
    "class LightGCN_Recommender(torch.nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64, num_layers=3, alpha=None):\n",
    "        super().__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        # The PyG LightGCN model will manage its own embeddings.\n",
    "        # Its parameters (including its internal nn.Embedding layer) will be registered\n",
    "        # when this LightGCN_Recommender module is instantiated.\n",
    "        self.lightgcn_model = LightGCN(\n",
    "            num_nodes=num_users + num_items, # Total number of user and item nodes\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_layers=num_layers,\n",
    "            alpha=alpha # If None, alpha is 1/(num_layers+1) for each layer, as in the paper\n",
    "        )\n",
    "        # No separate self.user_embedding or self.item_embedding in this wrapper.\n",
    "        # PyG's LightGCN initializes its internal nn.Embedding using xavier_uniform_ by default.\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        \"\"\"\n",
    "        Performs message passing using the internal LightGCN model to get final embeddings.\n",
    "        \"\"\"\n",
    "        # Ensure edge_index is of the correct type and contiguous for PyG operations.\n",
    "        current_edge_index = edge_index.long().contiguous()\n",
    "\n",
    "        # Call the get_embedding method of the PyG LightGCN model.\n",
    "        # This method uses the LightGCN model's own internal embedding layer\n",
    "        # and performs the light graph convolution.\n",
    "        final_embeddings = self.lightgcn_model.get_embedding(edge_index=current_edge_index)\n",
    "        \n",
    "        # Split the combined embeddings back into user and item embeddings\n",
    "        final_user_emb, final_item_emb = torch.split(final_embeddings, [self.num_users, self.num_items], dim=0)\n",
    "        return final_user_emb, final_item_emb\n",
    "\n",
    "    def predict_score(self, user_indices, item_indices, user_emb, item_emb):\n",
    "        \"\"\"\n",
    "        Predicts the interaction score between given users and items\n",
    "        based on their final embeddings.\n",
    "        \"\"\"\n",
    "        # Select embeddings for the given user and item indices\n",
    "        u_emb = user_emb[user_indices]\n",
    "        i_emb = item_emb[item_indices]\n",
    "        # Compute dot product\n",
    "        return torch.sum(u_emb * i_emb, dim=1)\n",
    "\n",
    "    def bpr_loss(self, user_emb, item_emb, pos_user_indices, pos_item_indices, neg_item_indices):\n",
    "        \"\"\"\n",
    "        Calculates the Bayesian Personalized Ranking (BPR) loss.\n",
    "        user_emb and item_emb are the final embeddings obtained from the forward pass.\n",
    "        \"\"\"\n",
    "        # Calculate scores for positive user-item pairs\n",
    "        pos_scores = self.predict_score(pos_user_indices, pos_item_indices, user_emb, item_emb)\n",
    "        # Calculate scores for negative user-item pairs (same users, different items)\n",
    "        neg_scores = self.predict_score(pos_user_indices, neg_item_indices, user_emb, item_emb)\n",
    "        \n",
    "        # BPR loss formula\n",
    "        loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-8).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 GraphSAGE (Graph Sample and Aggregate)\n",
    "\n",
    "**Rationale:**\n",
    "* **Inductive Learning:** Can generate embeddings for unseen nodes (though in this MovieLens setup, all nodes are typically seen during training, its architecture is inherently inductive).\n",
    "* **Neighborhood Aggregation:** Learns to aggregate feature information from a node's local neighborhood. Different aggregator functions (mean, GCN, LSTM, pooling) can be used.\n",
    "* **Flexibility:** Can incorporate node features if available, potentially leading to better performance when collaborative signals are sparse or for cold-start scenarios.\n",
    "\n",
    "For this implementation, we'll use learnable embeddings for users and items as initial features, similar to how LightGCN starts, and then apply GraphSAGE convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE_Recommender(torch.nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64, num_layers=2, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # User and Item embedding layers\n",
    "        self.user_embedding = torch.nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = torch.nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        # GraphSAGE convolution layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        # First layer: input is the initial embedding dimension\n",
    "        self.convs.append(SAGEConv(embedding_dim, embedding_dim))\n",
    "        # Subsequent layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(embedding_dim, embedding_dim))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(self.dropout_rate)\n",
    "\n",
    "        # Initialize weights\n",
    "        torch.nn.init.xavier_normal_(self.user_embedding.weight)\n",
    "        torch.nn.init.xavier_normal_(self.item_embedding.weight)\n",
    "\n",
    "    def get_initial_embeddings(self):\n",
    "        \"\"\"Concatenates initial user and item embeddings to form input node features.\"\"\"\n",
    "        return torch.cat([self.user_embedding.weight, self.item_embedding.weight], dim=0)\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        # Get initial node features (concatenated user and item embeddings)\n",
    "        x = self.get_initial_embeddings()\n",
    "        \n",
    "        # Apply GraphSAGE layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < self.num_layers - 1: # No activation/dropout after the last conv layer's output if used directly for dot product\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        \n",
    "        # Split back into user and item embeddings\n",
    "        final_user_emb, final_item_emb = torch.split(x, [self.num_users, self.num_items], dim=0)\n",
    "        return final_user_emb, final_item_emb\n",
    "\n",
    "    def predict_score(self, user_indices, item_indices, user_emb, item_emb):\n",
    "        u_emb = user_emb[user_indices]\n",
    "        i_emb = item_emb[item_indices]\n",
    "        return torch.sum(u_emb * i_emb, dim=1) # Dot product\n",
    "\n",
    "    def bpr_loss(self, user_emb, item_emb, pos_user_indices, pos_item_indices, neg_item_indices):\n",
    "        pos_scores = self.predict_score(pos_user_indices, pos_item_indices, user_emb, item_emb)\n",
    "        neg_scores = self.predict_score(pos_user_indices, neg_item_indices, user_emb, item_emb)\n",
    "        loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-8).mean()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model Configuration and Instantiation\n",
    "\n",
    "Choose the model you want to train and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing LightGCN Model...\n",
      "\n",
      "Model initialized and moved to device:\n",
      "LightGCN_Recommender(\n",
      "  (lightgcn_model): LightGCN(9746, 64, num_layers=3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# MODEL_CHOICE: 'LightGCN' or 'GraphSAGE'\n",
    "MODEL_CHOICE = 'LightGCN' # Change this to 'GraphSAGE' to use the other model\n",
    "\n",
    "# Model Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "NUM_LAYERS = 3    # For LightGCN: number of propagation layers.\n",
    "                  # For GraphSAGE: number of SAGEConv layers (e.g., 2 or 3 is common).\n",
    "GRAPHSAGE_DROPOUT = 0.1 # Dropout for GraphSAGE, not used by LightGCN here\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4 # L2 regularization for embeddings\n",
    "BATCH_SIZE = 2048 \n",
    "EPOCHS = 50 # Might need adjustment for ML-1M (e.g., fewer epochs if converges faster, or more if needed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if MODEL_CHOICE == 'LightGCN':\n",
    "    print(\"Initializing LightGCN Model...\")\n",
    "    model = LightGCN_Recommender(num_users, num_items, EMBEDDING_DIM, NUM_LAYERS).to(device)\n",
    "elif MODEL_CHOICE == 'GraphSAGE':\n",
    "    print(\"Initializing GraphSAGE Model...\")\n",
    "    # Note: NUM_LAYERS for GraphSAGE is typically 2-3. If using 3 for LightGCN, 2 might be a good start for GraphSAGE.\n",
    "    model = GraphSAGE_Recommender(num_users, num_items, EMBEDDING_DIM, num_layers=NUM_LAYERS, dropout_rate=GRAPHSAGE_DROPOUT).to(device)\n",
    "else:\n",
    "    raise ValueError(\"Invalid MODEL_CHOICE. Choose 'LightGCN' or 'GraphSAGE'.\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# The graph_data.edge_index is used for message passing and should be on the same device as the model\n",
    "graph_data = graph_data.to(device)\n",
    "print(\"\\nModel initialized and moved to device:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Training the Model\n",
    "\n",
    "**Objective:** Train the selected GNN model using BPR loss.\n",
    "\n",
    "**BPR Loss (Bayesian Personalized Ranking):**\n",
    "* Assumes users prefer items they interacted with over items they haven't.\n",
    "* For each positive user-item pair `(u, i+)`, a negative item `i-` (not interacted with by `u`) is sampled.\n",
    "* The loss aims to maximize the score difference between `(u, i+)` and `(u, i-)`.\n",
    "\n",
    "**Training Loop:**\n",
    "1.  Get current user and item embeddings from the GNN (after propagation on the full graph).\n",
    "2.  Sample a batch of positive interactions `(u, i+)` from `train_pos_edge_index`.\n",
    "3.  For each `(u, i+)`, sample a negative item `i-` (an item `u` has not interacted with).\n",
    "4.  Calculate BPR loss using the embeddings.\n",
    "5.  Backpropagate and update model parameters (embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for LightGCN model...\n",
      "Epoch 5/50, Model: LightGCN, Average BPR Loss: 0.6019\n",
      "Epoch 10/50, Model: LightGCN, Average BPR Loss: 0.6016\n",
      "Epoch 15/50, Model: LightGCN, Average BPR Loss: 0.6020\n",
      "Epoch 20/50, Model: LightGCN, Average BPR Loss: 0.6017\n",
      "Epoch 25/50, Model: LightGCN, Average BPR Loss: 0.6016\n",
      "Epoch 30/50, Model: LightGCN, Average BPR Loss: 0.6021\n",
      "Epoch 35/50, Model: LightGCN, Average BPR Loss: 0.6020\n",
      "Epoch 40/50, Model: LightGCN, Average BPR Loss: 0.6020\n",
      "Epoch 45/50, Model: LightGCN, Average BPR Loss: 0.6018\n",
      "Epoch 50/50, Model: LightGCN, Average BPR Loss: 0.6022\n",
      "Training finished for LightGCN model.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt # Import matplotlib\n",
    "# --- (your existing imports and code before this snippet) ---\n",
    "\n",
    "def sample_negative_items(user_idx, num_items, positive_items_for_user, num_samples=1):\n",
    "    \"\"\"Samples negative items for a given user.\"\"\"\n",
    "    neg_items = []\n",
    "    if not isinstance(positive_items_for_user, set):\n",
    "        positive_items_for_user_set = set(positive_items_for_user.tolist() if isinstance(positive_items_for_user, torch.Tensor) else positive_items_for_user)\n",
    "    else:\n",
    "        positive_items_for_user_set = positive_items_for_user\n",
    "\n",
    "    while len(neg_items) < num_samples:\n",
    "        sampled_item = random.randint(0, num_items - 1)\n",
    "        if sampled_item not in positive_items_for_user_set:\n",
    "            neg_items.append(sampled_item)\n",
    "    return torch.tensor(neg_items, dtype=torch.long)\n",
    "\n",
    "user_to_positive_items_train = train_df.groupby('user_idx')['item_idx'].apply(list).to_dict()\n",
    "\n",
    "print(f\"Starting training for {MODEL_CHOICE} model...\")\n",
    "\n",
    "# List to store average loss per epoch for plotting\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0 # Initialize as float\n",
    "    num_processed_samples = 0 # To correctly average loss if train_pos_edge_index.size(1) is 0\n",
    "\n",
    "    # Shuffle training data (positive edges)\n",
    "    shuffled_indices = torch.randperm(train_pos_edge_index.size(1))\n",
    "    shuffled_train_pos_edge_index = train_pos_edge_index[:, shuffled_indices]\n",
    "    \n",
    "    # Iterate over mini-batches\n",
    "    for i in range(0, shuffled_train_pos_edge_index.size(1), BATCH_SIZE):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Key Change: GNN forward pass (message passing) is now INSIDE the batch loop.\n",
    "        # This creates a fresh computation graph for each batch's backward pass.\n",
    "        # The model's parameters (embeddings, GNN weights) are updated by optimizer.step(),\n",
    "        # so subsequent calls to model() will use the updated parameters.\n",
    "        user_embeddings_batch, item_embeddings_batch = model(graph_data.edge_index)\n",
    "        \n",
    "        batch_pos_edges = shuffled_train_pos_edge_index[:, i:i+BATCH_SIZE]\n",
    "        pos_user_indices = batch_pos_edges[0, :].to(device)\n",
    "        pos_item_indices = batch_pos_edges[1, :].to(device)\n",
    "        \n",
    "        # Sample negative items for each positive pair in the batch\n",
    "        batch_neg_item_indices_list = []\n",
    "        for user_idx_val in pos_user_indices.cpu().tolist(): \n",
    "            positive_items_for_this_user = user_to_positive_items_train.get(user_idx_val, [])\n",
    "            # Ensure negative sampling is done correctly with item_idx values\n",
    "            neg_sample = sample_negative_items(user_idx_val, num_items, \n",
    "                                               set(positive_items_for_this_user), \n",
    "                                               num_samples=1) # Assuming BPR needs one negative sample per positive\n",
    "            batch_neg_item_indices_list.append(neg_sample)\n",
    "        \n",
    "        if not batch_neg_item_indices_list: \n",
    "            # This case should ideally not happen if pos_user_indices is not empty\n",
    "            continue\n",
    "\n",
    "        batch_neg_item_indices = torch.cat(batch_neg_item_indices_list).to(device)\n",
    "        \n",
    "        # Ensure batch_neg_item_indices has the same length as pos_user_indices\n",
    "        if len(batch_neg_item_indices) != len(pos_user_indices):\n",
    "            # This might happen if sampling issues occur or if a user has interacted with all items\n",
    "            # print(f\"Warning: Skipping batch due to mismatch in positive ({len(pos_user_indices)}) and negative ({len(batch_neg_item_indices)}) samples.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate BPR loss using the embeddings computed for THIS batch\n",
    "        loss = model.bpr_loss(user_embeddings_batch, item_embeddings_batch, \n",
    "                              pos_user_indices, pos_item_indices, \n",
    "                              batch_neg_item_indices)\n",
    "        \n",
    "        loss.backward() # Backpropagate for the current batch\n",
    "        optimizer.step() # Update model parameters\n",
    "        \n",
    "        total_loss += loss.item() * pos_user_indices.size(0) # Accumulate weighted loss\n",
    "        num_processed_samples += pos_user_indices.size(0)\n",
    "        \n",
    "    if num_processed_samples > 0:\n",
    "        avg_loss = total_loss / num_processed_samples\n",
    "    else:\n",
    "        avg_loss = 0.0 # Handle case with no processed samples (e.g., empty train_pos_edge_index)\n",
    "        \n",
    "    epoch_losses.append(avg_loss) # Store average loss for this epoch\n",
    "\n",
    "    if (epoch + 1) % 5 == 0: # Or any other frequency for printing\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Model: {MODEL_CHOICE}, Average BPR Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"Training finished for {MODEL_CHOICE} model.\")\n",
    "\n",
    "# Plotting the training loss\n",
    "if EPOCHS > 0 and epoch_losses: # Ensure there's data to plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', linestyle='-')\n",
    "    plt.title(f'Training Loss per Epoch ({MODEL_CHOICE})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average BPR Loss')\n",
    "    # Ensure xticks match the number of epochs plotted\n",
    "    plotted_epochs = len(epoch_losses)\n",
    "    if plotted_epochs > 0:\n",
    "        plt.xticks(range(1, plotted_epochs + 1, max(1, plotted_epochs // 10))) # Adjust tick frequency for many epochs\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training epochs were run or no losses recorded, skipping plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Evaluation\n",
    "\n",
    "**Objective:** Evaluate the trained model on the test set using common recommendation metrics.\n",
    "\n",
    "**Metrics:**\n",
    "* **Precision@K:** Proportion of recommended items in the top-K set that are relevant.\n",
    "* **Recall@K:** Proportion of relevant items that are successfully recommended in the top-K set.\n",
    "* **NDCG@K (Normalized Discounted Cumulative Gain):** Considers the position of hits in the top-K list.\n",
    "\n",
    "**Evaluation Process:**\n",
    "1.  Get the final learned user and item embeddings.\n",
    "2.  For each user in the test set:\n",
    "    a.  Predict scores for all items (or a large set of unobserved items + true positive test items).\n",
    "    b.  Rank items based on scores.\n",
    "    c.  Compare top-K recommendations against the user's actual positive interactions in the test set.\n",
    "3.  Average metrics across all test users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating LightGCN model...\n",
      "Evaluation Results for LightGCN (K=20):\n",
      "  Precision@20: 0.1566\n",
      "  Recall@20:    0.1137\n",
      "  NDCG@20:      0.1955\n"
     ]
    }
   ],
   "source": [
    "def get_user_positive_items_test(test_df_user):\n",
    "    \"\"\"Returns a set of item_idx that the user has positively interacted with in the test set.\"\"\"\n",
    "    return set(test_df_user['item_idx'].tolist())\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model_to_eval, graph_data_eval, test_df_eval, user_to_train_items_map, num_users_eval, num_items_eval, K=20, device_eval='cpu'):\n",
    "    model_to_eval.eval()\n",
    "    \n",
    "    final_user_emb, final_item_emb = model_to_eval(graph_data_eval.edge_index)\n",
    "    final_user_emb = final_user_emb.to(device_eval)\n",
    "    final_item_emb = final_item_emb.to(device_eval)\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "\n",
    "    test_users = test_df_eval['user_idx'].unique()\n",
    "\n",
    "    for user_idx in test_users:\n",
    "        train_pos_items = set(user_to_train_items_map.get(user_idx, []))\n",
    "        true_pos_items_test = get_user_positive_items_test(test_df_eval[test_df_eval['user_idx'] == user_idx])\n",
    "        if not true_pos_items_test:\n",
    "            continue\n",
    "\n",
    "        user_embedding_single = final_user_emb[user_idx].unsqueeze(0)\n",
    "        all_item_scores = torch.matmul(user_embedding_single, final_item_emb.T).squeeze()\n",
    "        \n",
    "        for train_item_idx in train_pos_items:\n",
    "            if 0 <= train_item_idx < num_items_eval:\n",
    "                 all_item_scores[train_item_idx] = -torch.inf\n",
    "        \n",
    "        _, top_k_indices = torch.topk(all_item_scores, k=K)\n",
    "        recommended_items = set(top_k_indices.cpu().tolist())\n",
    "\n",
    "        hits = recommended_items.intersection(true_pos_items_test)\n",
    "        \n",
    "        precision_at_k = len(hits) / K if K > 0 else 0\n",
    "        precisions.append(precision_at_k)\n",
    "        \n",
    "        recall_at_k = len(hits) / len(true_pos_items_test) if len(true_pos_items_test) > 0 else 0\n",
    "        recalls.append(recall_at_k)\n",
    "        \n",
    "        relevance = torch.zeros(K, device=device_eval)\n",
    "        for i, item_r_idx in enumerate(top_k_indices.tolist()):\n",
    "            if item_r_idx in true_pos_items_test:\n",
    "                relevance[i] = 1\n",
    "        \n",
    "        ideal_relevance_len = min(K, len(true_pos_items_test))\n",
    "        idcg_val = torch.sum(1.0 / torch.log2(torch.arange(2, ideal_relevance_len + 2, device=device_eval).float()))\n",
    "        dcg_val = torch.sum(relevance / torch.log2(torch.arange(2, K + 2, device=device_eval).float()))\n",
    "        ndcg_at_k = (dcg_val / idcg_val).item() if idcg_val > 0 else 0\n",
    "        ndcgs.append(ndcg_at_k)\n",
    "\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    avg_ndcg = np.mean(ndcgs) if ndcgs else 0\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_ndcg\n",
    "\n",
    "print(f\"\\nEvaluating {MODEL_CHOICE} model...\")\n",
    "eval_precision, eval_recall, eval_ndcg = evaluate_model(model, graph_data, test_df, user_to_positive_items_train, num_users, num_items, K=20, device_eval=device)\n",
    "\n",
    "print(f\"Evaluation Results for {MODEL_CHOICE} (K=20):\")\n",
    "print(f\"  Precision@20: {eval_precision:.4f}\")\n",
    "print(f\"  Recall@20:    {eval_recall:.4f}\")\n",
    "print(f\"  NDCG@20:      {eval_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Example: Generating Recommendations for a Sample User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating recommendations for sample user_id: 621 (user_idx: 620) using LightGCN model\n",
      "  User liked in training (first 5, not recommended): [2606, 1721, 2580, 382, 2560]\n",
      "  User liked in test (ground truth, first 5): [1623, 2355, 1120, 2762, 2770]\n",
      "\n",
      "  Top 10 Recommended item_ids for user 621 (using LightGCN): [ 260 1210 2028 2571 1270  593 1198  608 2762 1197]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def recommend_for_user(user_idx_target, model_eval, graph_data_rec, user_to_train_items_map_rec, num_items_rec, item_encoder_rec, K=10, device_rec='cpu'):\n",
    "    model_eval.eval()\n",
    "    final_user_emb_rec, final_item_emb_rec = model_eval(graph_data_rec.edge_index)\n",
    "    final_user_emb_rec = final_user_emb_rec.to(device_rec)\n",
    "    final_item_emb_rec = final_item_emb_rec.to(device_rec)\n",
    "\n",
    "    train_pos_items_rec = set(user_to_train_items_map_rec.get(user_idx_target, []))\n",
    "    \n",
    "    user_embedding_single_rec = final_user_emb_rec[user_idx_target].unsqueeze(0)\n",
    "    all_item_scores_rec = torch.matmul(user_embedding_single_rec, final_item_emb_rec.T).squeeze()\n",
    "    \n",
    "    for train_item_idx_rec in train_pos_items_rec:\n",
    "        if 0 <= train_item_idx_rec < num_items_rec:\n",
    "            all_item_scores_rec[train_item_idx_rec] = -torch.inf\n",
    "            \n",
    "    _, top_k_indices_rec = torch.topk(all_item_scores_rec, k=K)\n",
    "    recommended_item_indices = top_k_indices_rec.cpu().tolist()\n",
    "    \n",
    "    try:\n",
    "        recommended_original_ids = item_encoder_rec.inverse_transform(recommended_item_indices)\n",
    "    except IndexError:\n",
    "        print(f\"Warning: Could not inverse_transform some item indices. Max index: {max(recommended_item_indices)}, encoder classes: {len(item_encoder_rec.classes_)}\")\n",
    "        recommended_original_ids = [f\"item_idx_{idx}\" for idx in recommended_item_indices] \n",
    "\n",
    "    return recommended_original_ids\n",
    "\n",
    "if not test_df.empty:\n",
    "    sample_user_original_id = test_df['user_id'].sample(1).iloc[0]\n",
    "    sample_user_idx = user_encoder.transform([sample_user_original_id])[0]\n",
    "    print(f\"\\nGenerating recommendations for sample user_id: {sample_user_original_id} (user_idx: {sample_user_idx}) using {MODEL_CHOICE} model\")\n",
    "\n",
    "    known_positives_train_orig_ids = []\n",
    "    if sample_user_idx in user_to_positive_items_train:\n",
    "        known_positives_train_indices = user_to_positive_items_train[sample_user_idx][:5]\n",
    "        try:\n",
    "           known_positives_train_orig_ids = item_encoder.inverse_transform(known_positives_train_indices)\n",
    "        except IndexError:\n",
    "            known_positives_train_orig_ids = [f\"item_idx_{idx}\" for idx in known_positives_train_indices]\n",
    "    print(f\"  User liked in training (first 5, not recommended): {list(known_positives_train_orig_ids)}\")\n",
    "\n",
    "    true_positives_test_orig_ids = []\n",
    "    user_test_data = test_df[test_df['user_idx'] == sample_user_idx]\n",
    "    if not user_test_data.empty:\n",
    "        true_positives_test_indices = user_test_data['item_idx'].unique()[:5]\n",
    "        try:\n",
    "            true_positives_test_orig_ids = item_encoder.inverse_transform(true_positives_test_indices)\n",
    "        except IndexError:\n",
    "            true_positives_test_orig_ids = [f\"item_idx_{idx}\" for idx in true_positives_test_indices]\n",
    "    print(f\"  User liked in test (ground truth, first 5): {list(true_positives_test_orig_ids)}\")\n",
    "\n",
    "    recommendations = recommend_for_user(sample_user_idx, model, graph_data, user_to_positive_items_train, num_items, item_encoder, K=10, device_rec=device)\n",
    "    print(f\"\\n  Top 10 Recommended item_ids for user {sample_user_original_id} (using {MODEL_CHOICE}): {recommendations}\")\n",
    "else:\n",
    "    print(\"\\nTest dataframe is empty, cannot generate sample recommendations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Discussion, Critical Analysis, and Next Steps\n",
    "\n",
    "**Objective:** Reflect on the project, its limitations, and potential improvements. This is crucial for an interview setting.\n",
    "\n",
    "### 6.1 Model Performance and Limitations\n",
    "* **Performance:** The achieved Precision/Recall/NDCG values provide a quantitative measure. Compare results between LightGCN and GraphSAGE. Note any differences in training time or convergence. Compare against standard benchmarks for MovieLens 1M if available (though exact setup varies).\n",
    "* **Dataset:** MovieLens 1M is a good benchmark but differs from short video platforms:\n",
    "    * **Interaction Types:** TikTok has richer interactions. We modeled only positive interactions (ratings).\n",
    "    * **Content Features:** We haven't used explicit movie/user features (genres, demographics) in these GNNs yet, though GraphSAGE is well-suited for them. LightGCN often performs best with only collaborative signals.\n",
    "    * **Social Graph:** TikTok has a strong social component, not modeled here.\n",
    "    * **Dynamic Nature:** Our models are static once trained.\n",
    "* **Cold Start:** Both LightGCN and this GraphSAGE variant (relying on learned ID embeddings) suffer from cold start for new users/items.\n",
    "* **Scalability:** Scaling to TikTok's level requires distributed training, graph partitioning, and efficient embedding stores.\n",
    "* **Negative Sampling:** Our random negative sampling is basic. More advanced techniques can improve performance.\n",
    "* **GraphSAGE Aggregation:** We used the default mean aggregator in SAGEConv. Exploring other aggregators (LSTM, pooling) could yield different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Potential Improvements and Future Work (TikTok Context)\n",
    "\n",
    "1.  **Richer Graph Schema (Heterogeneous Graph):**\n",
    "    * **Nodes:** `User`, `Video`, `Creator`, `Topic`/`Hashtag`, `Sound`.\n",
    "    * **Edges:** `User-watches-Video` (with features like watch_time_%), `User-likes-Video`, `User-shares-Video`, `User-follows-Creator`, `Video-has_topic-Topic`, `Video-uses_sound-Sound`, `User-follows-User`.\n",
    "    * **Model:** Use GNNs designed for heterogeneous graphs like RGCN, HAN, or `HeteroConv` in PyG. This would be a natural next step for models like GraphSAGE or GAT.\n",
    "\n",
    "2.  **Incorporate Content Features:**\n",
    "    * Load movie genres from `movies.dat` (for MovieLens) or extract embeddings from video frames, audio, text for a TikTok-like scenario.\n",
    "    * For GraphSAGE/GAT: Concatenate these features with or use them as initial node features. This can help with cold-start and content-aware recommendations.\n",
    "    * For LightGCN: Generally, it's found to perform best without explicit features, but hybrid versions exist.\n",
    "\n",
    "3.  **Advanced GNN Architectures:**\n",
    "    * **GAT (Graph Attention Network):** Implement GAT to allow nodes to weigh the importance of neighbors, adding more expressiveness. This is a strong candidate for the next model to try.\n",
    "    * **PinSage (Pinterest):** Study and potentially implement key ideas from PinSage, designed for web-scale recommendation (e.g., random walk-based convolutions, multi-hop sampling).\n",
    "\n",
    "4.  **Temporal Dynamics:**\n",
    "    * Incorporate timestamps of interactions more directly into the GNN, perhaps using Temporal Graph Networks (TGNs) or by creating time-aware node/edge features.\n",
    "\n",
    "5.  **Advanced Negative Sampling:**\n",
    "    * Implement more sophisticated negative sampling strategies: \n",
    "        * **Popularity-based sampling:** Sample popular items less frequently as negatives.\n",
    "        * **Hard negative mining:** Sample items that the model currently scores high but are not true positives.\n",
    "        * **In-batch negatives:** Use other items within the same batch as negatives.\n",
    "\n",
    "6.  **Multi-Task Learning:** Predict multiple engagement types simultaneously.\n",
    "\n",
    "7.  **Scalability Solutions:** Discuss graph sampling (NeighborSampler, GraphSAINT), distributed training (PyTorch Distributed, DistDGL), and embedding serving (Faiss).\n",
    "\n",
    "8.  **Explainability:** Explore methods like GNNExplainer to understand model predictions.\n",
    "\n",
    "9.  **Hyperparameter Optimization:** Use tools like Optuna or Ray Tune for systematic hyperparameter search for embedding dimensions, number of layers, learning rates, etc., for each GNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4636d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amanp\\anaconda3\\Lib\\site-packages\\nbformat\\__init__.py:93: MissingIDFieldWarning: Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
      "  validate(nb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook converted to: GNN_Recommendation_System.html\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "from nbconvert import HTMLExporter\n",
    "import os\n",
    "\n",
    "def convert_this_notebook_to_html():\n",
    "    notebook_path = 'GNN_Recommendation_System.ipynb'  # Update if different\n",
    "    output_html_path = notebook_path.replace('.ipynb', '.html')\n",
    "\n",
    "    if not os.path.exists(notebook_path):\n",
    "        print(f\"Notebook not found: {notebook_path}\")\n",
    "        return\n",
    "\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook_node = nbformat.read(f, as_version=4)\n",
    "\n",
    "    html_exporter = HTMLExporter()\n",
    "    (body, _) = html_exporter.from_notebook_node(notebook_node)\n",
    "\n",
    "    with open(output_html_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(body)\n",
    "\n",
    "    print(f\"Notebook converted to: {output_html_path}\")\n",
    "\n",
    "convert_this_notebook_to_html()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
